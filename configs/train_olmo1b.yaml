# =============================
# Training configuration
# =============================

# model
model_name: "allenai/OLMo-2-0425-1B"
local_path: "./models/OLMo-2-0425-1B-full"

# data
datasets:
  - name: "hendrycks/ethics"
    config: "deontology"
    split: "train"
    text_field: "text"
  - name: "hendrycks/ethics"
    config: "justice"
    split: "train"
    text_field: "text"
  # - name: "hendrycks/ethics"
  #   config: "virtue"
  #   split: "train"
  #   text_field: "text"

# adapter parameters
rank: 8
alpha: 16.0
dropout: 0.05
target_modules:
  - "model.layers.8.mlp.down_proj"
  - "model.layers.15.mlp.down_proj"

# training hyperparameters
batch_size: 4
lr: 5e-4
epochs: 1
max_length: 512
max_train_samples: 2000
save_every: 1
gradient_accumulation_steps: 4
num_workers: 6
num_proc: 12

# misc
runs_dir: "./runs"
data_dir: "./data"
